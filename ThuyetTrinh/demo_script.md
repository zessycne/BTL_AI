# DEMO SCRIPT - H∆Ø·ªöNG D·∫™N THUY·∫æT TR√åNH

## **PH·∫¶N 1: GI·ªöI THI·ªÜU (2 ph√∫t)**

### **M·ªü ƒë·∫ßu:**
"Ch√†o th·∫ßy/c√¥ v√† c√°c b·∫°n. H√¥m nay t√¥i s·∫Ω tr√¨nh b√†y b√†i t·∫≠p l·ªõn v·ªÅ **H·ªá th·ªëng nh·∫≠n di·ªán th∆∞ r√°c (Spam Detection)** s·ª≠ d·ª•ng Machine Learning."

### **T·ªïng quan d·ª± √°n:**
- **M·ª•c ti√™u:** X√¢y d·ª±ng h·ªá th·ªëng t·ª± ƒë·ªông ph√¢n lo·∫°i email spam/ham
- **Dataset:** SMS Spam Collection Dataset (5,574 messages)
- **Approach:** SentenceTransformer + LogisticRegression v·ªõi code t·ªëi ∆∞u h√≥a
- **Technologies:** Python, scikit-learn, SentenceTransformer, Tkinter

### **C·∫•u tr√∫c d·ª± √°n:**
```
DemoAI/
‚îú‚îÄ‚îÄ tien_xu_ly.py          # Data preprocessing
‚îú‚îÄ‚îÄ mo_hinh.py             # Optimized model training (SentenceTransformer)
‚îú‚îÄ‚îÄ du_doan_email.py       # Command line prediction
‚îú‚îÄ‚îÄ ui_du_doan_email.py    # User interface
‚îú‚îÄ‚îÄ spam.csv               # Dataset
‚îú‚îÄ‚îÄ mo_hinh_spam.pkl       # Trained model
‚îî‚îÄ‚îÄ sentence_model.txt     # SentenceTransformer model info
```

---

## **PH·∫¶N 2: DEMO SENTENCETRANSFORMER APPROACH (4 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y training**
```bash
python mo_hinh.py
```

### **Gi·∫£i th√≠ch qu√° tr√¨nh:**
"T√¥i s·∫Ω demo approach s·ª≠ d·ª•ng SentenceTransformer + LogisticRegression v·ªõi code ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a:"

1. **Data preprocessing:** ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV, x·ª≠ l√Ω encoding
2. **Text cleaning:** L√†m s·∫°ch text v·ªõi h√†m `clean_text_list()`
3. **Feature extraction:** S·ª≠ d·ª•ng SentenceTransformer ƒë·ªÉ t·∫°o embeddings
4. **Batch processing:** X·ª≠ l√Ω theo batch ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ
5. **Model training:** Logistic Regression v·ªõi max_iter=1000
6. **Evaluation:** T√≠nh c√°c metrics (accuracy, precision, recall, F1-score)
7. **Model saving:** L∆∞u m√¥ h√¨nh v√† th√¥ng tin embedder

### **K·∫øt qu·∫£ mong ƒë·ª£i:**
```
=== Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi SentenceTransformer ===
ƒê·ªô ch√≠nh x√°c: 0.9856
B√°o c√°o ph√¢n lo·∫°i:
              precision    recall  f1-score   support

Kh√¥ng ph·∫£i r√°c       0.99      0.99      0.99       966
Th∆∞ r√°c             0.97      0.97      0.97       149

    accuracy                           0.99      1115
   macro avg       0.98      0.98      0.98      1115
weighted avg       0.99      0.99      0.99      1115

ƒê√£ l∆∞u m√¥ h√¨nh v√†o mo_hinh_spam.pkl v√† t√™n model SentenceTransformer v√†o sentence_model.txt
```

### **Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Accuracy 98.56%:** R·∫•t t·ªët cho b√†i to√°n spam detection
- **Precision cao:** √çt false positive (kh√¥ng block nh·∫ßm email quan tr·ªçng)
- **Training time:** Kho·∫£ng 3-5 ph√∫t, ph√π h·ª£p v·ªõi ƒë·ªô ph·ª©c t·∫°p
- **Code t·ªëi ∆∞u:** ƒê√£ lo·∫°i b·ªè c√°c h√†m tr√πng l·∫∑p, d·ªÖ maintain

---

## **PH·∫¶N 3: DEMO USER INTERFACE (3 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y UI**
```bash
python ui_du_doan_email.py
```

### **Demo v·ªõi email m·∫´u:**

**Email spam m·∫´u:**
```
Subject: URGENT: You've won $1,000,000!
Body: Congratulations! You've been selected to receive $1,000,000. 
Click here to claim your prize: http://fake-spam-link.com
This is a limited time offer. Don't miss out!
```

**Email ham m·∫´u:**
```
Subject: Meeting tomorrow
Body: Hi team,
Just a reminder that we have a meeting tomorrow at 2 PM in the conference room.
Please prepare your quarterly reports.
Best regards,
John
```

### **Gi·∫£i th√≠ch UI:**
- **User-friendly interface:** D·ªÖ s·ª≠ d·ª•ng v·ªõi Tkinter
- **Real-time prediction:** K·∫øt qu·∫£ hi·ªÉn th·ªã ngay l·∫≠p t·ª©c
- **Error handling:** Th√¥ng b√°o l·ªói th√¢n thi·ªán
- **Clear instructions:** H∆∞·ªõng d·∫´n r√µ r√†ng

---

## **PH·∫¶N 4: DEMO COMMAND LINE (2 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y command line tool**
```bash
python du_doan_email.py
```

### **Demo v·ªõi email m·∫´u:**
```
Nh·∫≠p email (g√µ 'END' ƒë·ªÉ k·∫øt th√∫c):
> Hi, can you send me the meeting notes from yesterday?

K·∫øt qu·∫£: Kh√¥ng spam

Nh·∫≠p email (g√µ 'END' ƒë·ªÉ k·∫øt th√∫c):
> FREE VIAGRA NOW!!! Click here to get your free pills!!!

K·∫øt qu·∫£: Spam

Nh·∫≠p email (g√µ 'END' ƒë·ªÉ k·∫øt th√∫c):
> END
```

### **Gi·∫£i th√≠ch:**
- **Flexible input:** H·ªó tr·ª£ nh·∫≠p email nhi·ªÅu d√≤ng
- **Batch processing:** X·ª≠ l√Ω hi·ªáu qu·∫£ v·ªõi SentenceTransformer
- **Clear output:** K·∫øt qu·∫£ d·ªÖ hi·ªÉu

---

## **PH·∫¶N 5: CODE OPTIMIZATION HIGHLIGHTS (2 ph√∫t)**

### **T·ªëi ∆∞u h√≥a ƒë√£ th·ª±c hi·ªán:**

#### **1. Lo·∫°i b·ªè h√†m tr√πng l·∫∑p:**
```python
# ƒê√£ lo·∫°i b·ªè:
# - encode_sentences() (tr√πng v·ªõi batch_encode())
# - xay_dung_va_danh_gia_mo_hinh() (tr√πng v·ªõi train_and_evaluate())
```

#### **2. C·∫•u tr√∫c code r√µ r√†ng:**
```python
# C√°c h√†m chuy√™n bi·ªát:
def huan_luyen_mo_hinh(X_train_emb, y_train):     # Ch·ªâ hu·∫•n luy·ªán
def danh_gia_mo_hinh(mo_hinh, X_test_emb, y_test): # Ch·ªâ ƒë√°nh gi√°
def du_doan_tin_nhan(mo_hinh, embedder, tin_nhan): # Ch·ªâ d·ª± ƒëo√°n
def train_and_evaluate(...):                       # Pipeline ch√≠nh
```

#### **3. Batch processing hi·ªáu qu·∫£:**
```python
def batch_encode(model, texts, batch_size=128):
    """Encode embedding theo batch nh·ªè ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        emb = model.encode(batch, show_progress_bar=False)
        embeddings.append(emb)
    return np.vstack(embeddings)
```

#### **4. Error handling t·ªët:**
```python
def clean_text_list(series):
    return [str(s) if pd.notnull(s) and str(s).strip() != "" else "[EMPTY]" for s in series]
```

---

## **PH·∫¶N 6: SO S√ÅNH V√Ä K·∫æT LU·∫¨N (2 ph√∫t)**

### **∆Øu ƒëi·ªÉm c·ªßa approach hi·ªán t·∫°i:**

| Aspect | SentenceTransformer + LR |
|--------|-------------------------|
| Accuracy | 98.56% |
| Precision | 97% |
| Recall | 97% |
| F1-score | 97% |
| Semantic understanding | Cao |
| Code maintainability | Cao |
| Memory efficiency | T·ªët (batch processing) |
| Error handling | T·ªët |

### **∆Øu nh∆∞·ª£c ƒëi·ªÉm:**

**SentenceTransformer + Logistic Regression:**
- ‚úÖ Hi·ªÉu ng·ªØ nghƒ©a t·ªët h∆°n
- ‚úÖ Accuracy cao (98.56%)
- ‚úÖ Code t·ªëi ∆∞u, d·ªÖ maintain
- ‚úÖ Batch processing hi·ªáu qu·∫£
- ‚úÖ Error handling t·ªët
- ‚ùå Training time l√¢u h∆°n (3-5 ph√∫t)
- ‚ùå Memory usage cao h∆°n

### **Recommendations:**
1. **Production:** S·ª≠ d·ª•ng approach n√†y cho accuracy cao
2. **Development:** Code modular d·ªÖ m·ªü r·ªông
3. **Maintenance:** Code s·∫°ch, √≠t tr√πng l·∫∑p
4. **Performance:** Batch processing t·ªëi ∆∞u

---

## **PH·∫¶N 7: Q&A PREPARATION**

### **C√¢u h·ªèi th∆∞·ªùng g·∫∑p:**

**Q: "T·∫°i sao ch·ªçn SentenceTransformer?"**
A: "SentenceTransformer hi·ªÉu ng·ªØ nghƒ©a s√¢u s·∫Øc h∆°n TF-IDF, ph√π h·ª£p cho vi·ªác ph√¢n lo·∫°i email spam v√¨ c√≥ th·ªÉ hi·ªÉu context v√† √Ω nghƒ©a th·ª±c s·ª± c·ªßa tin nh·∫Øn."

**Q: "L√†m sao c·∫£i thi·ªán model?"**
A: "C√≥ th·ªÉ th·ª≠: 1) Ensemble methods k·∫øt h·ª£p nhi·ªÅu models, 2) Deep learning (LSTM/BERT), 3) Feature engineering t·ªët h∆°n, 4) Data augmentation, 5) Hyperparameter tuning."

**Q: "Code c√≥ t·ªëi ∆∞u kh√¥ng?"**
A: "ƒê√£ t·ªëi ∆∞u b·∫±ng c√°ch: 1) Lo·∫°i b·ªè h√†m tr√πng l·∫∑p, 2) Batch processing, 3) Modular design, 4) Error handling t·ªët, 5) Memory management hi·ªáu qu·∫£."

**Q: "L√†m sao deploy production?"**
A: "1) API development v·ªõi Flask/FastAPI, 2) Docker containerization, 3) Cloud deployment (AWS/GCP), 4) Monitoring v√† logging, 5) Auto-scaling."

---

## **PH·∫¶N 8: TECHNICAL DETAILS**

### **Code highlights:**

**Optimized training pipeline (mo_hinh.py):**
```python
def train_and_evaluate(duong_dan_file: str, duong_dan_mo_hinh: str, duong_dan_embedder: str):
    """Pipeline: train, test, l∆∞u m√¥ h√¨nh v√† t√™n model embedding."""
    # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    X_train, X_test, y_train, y_test = doc_va_tien_xu_ly_du_lieu(duong_dan_file)
    
    # Kh·ªüi t·∫°o SentenceTransformer
    embedder = SentenceTransformer(MODEL_NAME)
    
    # Ti·ªÅn x·ª≠ l√Ω v√† encode d·ªØ li·ªáu
    X_train_clean = clean_text_list(X_train)
    X_test_clean = clean_text_list(X_test)
    X_train_emb = batch_encode(embedder, X_train_clean)
    X_test_emb = batch_encode(embedder, X_test_clean)
    
    # Hu·∫•n luy·ªán m√¥ h√¨nh
    mo_hinh = huan_luyen_mo_hinh(X_train_emb, y_train)
    
    # ƒê√°nh gi√° m√¥ h√¨nh
    do_chinh_xac, bao_cao = danh_gia_mo_hinh(mo_hinh, X_test_emb, y_test)
    
    # L∆∞u m√¥ h√¨nh
    luu_mo_hinh_va_embedder(mo_hinh, duong_dan_mo_hinh, duong_dan_embedder)
    
    return mo_hinh, embedder
```

**Efficient batch processing:**
```python
def batch_encode(model, texts, batch_size=128):
    """Encode embedding theo batch nh·ªè ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        emb = model.encode(batch, show_progress_bar=False)
        embeddings.append(emb)
    return np.vstack(embeddings)
```

**Modular prediction function:**
```python
def du_doan_tin_nhan(mo_hinh, embedder, tin_nhan: str):
    """D·ª± ƒëo√°n m·ªôt tin nh·∫Øn/email l√† spam hay kh√¥ng spam."""
    tin_nhan_clean = clean_text_list([tin_nhan])
    tin_nhan_emb = batch_encode(embedder, tin_nhan_clean)
    du_doan = mo_hinh.predict(tin_nhan_emb)[0]
    return "Spam" if du_doan == 1 else "Kh√¥ng spam"
```

---

## **PH·∫¶N 9: CONCLUSION**

### **T√≥m t·∫Øt:**
- ‚úÖ X√¢y d·ª±ng th√†nh c√¥ng h·ªá th·ªëng spam detection v·ªõi accuracy 98.56%
- ‚úÖ Code t·ªëi ∆∞u h√≥a, lo·∫°i b·ªè tr√πng l·∫∑p, d·ªÖ maintain
- ‚úÖ Batch processing hi·ªáu qu·∫£, tr√°nh tr√†n b·ªô nh·ªõ
- ‚úÖ User interface th√¢n thi·ªán
- ‚úÖ Error handling t·ªët
- ‚úÖ Modular design cho d·ªÖ m·ªü r·ªông

### **Future work:**
- Ensemble methods
- Deep learning approaches
- Production deployment
- Real-time monitoring
- Continuous learning

### **Thank you:**
"C·∫£m ∆°n th·∫ßy/c√¥ v√† c√°c b·∫°n ƒë√£ l·∫Øng nghe. T√¥i s·∫µn s√†ng tr·∫£ l·ªùi c√°c c√¢u h·ªèi."

---

## **CHECKLIST TR∆Ø·ªöC KHI DEMO:**

- [ ] Test t·∫•t c·∫£ code tr∆∞·ªõc khi demo
- [ ] Chu·∫©n b·ªã email m·∫´u (spam v√† ham)
- [ ] Backup d·ªØ li·ªáu v√† models
- [ ] Practice demo nhi·ªÅu l·∫ßn
- [ ] Chu·∫©n b·ªã answers cho Q&A
- [ ] Test UI tr√™n m√°y kh√°c
- [ ] Backup presentation materials
- [ ] Chu·∫©n b·ªã slides ho·∫∑c notes

**Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ** 