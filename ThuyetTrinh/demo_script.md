# DEMO SCRIPT - H∆Ø·ªöNG D·∫™N THUY·∫æT TR√åNH

## **PH·∫¶N 1: GI·ªöI THI·ªÜU (2 ph√∫t)**

### **M·ªü ƒë·∫ßu:**
"Ch√†o th·∫ßy/c√¥ v√† c√°c b·∫°n. H√¥m nay t√¥i s·∫Ω tr√¨nh b√†y b√†i t·∫≠p l·ªõn v·ªÅ **H·ªá th·ªëng nh·∫≠n di·ªán th∆∞ r√°c (Spam Detection)** s·ª≠ d·ª•ng Machine Learning."

### **T·ªïng quan d·ª± √°n:**
- **M·ª•c ti√™u:** X√¢y d·ª±ng h·ªá th·ªëng t·ª± ƒë·ªông ph√¢n lo·∫°i email spam/ham
- **Dataset:** SMS Spam Collection Dataset (5,574 messages)
- **Approaches:** 2 ph∆∞∆°ng ph√°p kh√°c nhau ƒë·ªÉ so s√°nh hi·ªáu qu·∫£
- **Technologies:** Python, scikit-learn, SentenceTransformer, Tkinter

### **C·∫•u tr√∫c d·ª± √°n:**
```
DemoAI/
‚îú‚îÄ‚îÄ tien_xu_ly.py          # Data preprocessing
‚îú‚îÄ‚îÄ dac_trung.py           # TF-IDF feature extraction
‚îú‚îÄ‚îÄ mo_hinh.py             # SentenceTransformer approach
‚îú‚îÄ‚îÄ mo_hinh_1.py           # TF-IDF approach
‚îú‚îÄ‚îÄ du_doan.py             # Prediction pipeline
‚îî‚îÄ‚îÄ ui_du_doan_email.py    # User interface
```

---

## **PH·∫¶N 2: DEMO TF-IDF APPROACH (3 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y training**
```bash
python mo_hinh_1.py
```

### **Gi·∫£i th√≠ch qu√° tr√¨nh:**
"ƒê·∫ßu ti√™n, t√¥i s·∫Ω demo approach s·ª≠ d·ª•ng TF-IDF + Logistic Regression:"

1. **Data preprocessing:** ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV, x·ª≠ l√Ω encoding
2. **Feature extraction:** S·ª≠ d·ª•ng TF-IDF v·ªõi n-gram (1,2) v√† max_features=3000
3. **Model training:** Logistic Regression v·ªõi max_iter=1000
4. **Evaluation:** T√≠nh c√°c metrics (accuracy, precision, recall, F1-score)

### **K·∫øt qu·∫£ mong ƒë·ª£i:**
```
ƒê·ªô ch√≠nh x√°c: 0.9745
B√°o c√°o ph√¢n lo·∫°i:
              precision    recall  f1-score   support

Kh√¥ng spam       0.98      0.98      0.98       966
Spam             0.95      0.95      0.95       149

    accuracy                           0.97      1115
   macro avg       0.96      0.96      0.96      1115
weighted avg       0.97      0.97      0.97      1115

Th·ªùi gian ch·∫°y: 15.23 gi√¢y
```

### **Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Accuracy 97.45%:** R·∫•t t·ªët cho b√†i to√°n spam detection
- **Precision cao:** √çt false positive (kh√¥ng block nh·∫ßm email quan tr·ªçng)
- **Training time:** Ch·ªâ 15 gi√¢y, r·∫•t nhanh

---

## **PH·∫¶N 3: DEMO SENTENCETRANSFORMER APPROACH (3 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y training**
```bash
python mo_hinh.py
```

### **Gi·∫£i th√≠ch qu√° tr√¨nh:**
"Ti·∫øp theo, t√¥i s·∫Ω demo approach s·ª≠ d·ª•ng SentenceTransformer + Logistic Regression:"

1. **Data preprocessing:** T∆∞∆°ng t·ª± nh∆∞ tr√™n
2. **Feature extraction:** S·ª≠ d·ª•ng SentenceTransformer ƒë·ªÉ t·∫°o embeddings
3. **Batch processing:** X·ª≠ l√Ω theo batch ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ
4. **Model training:** Logistic Regression tr√™n embeddings
5. **Evaluation:** So s√°nh v·ªõi TF-IDF approach

### **K·∫øt qu·∫£ mong ƒë·ª£i:**
```
ƒê·ªô ch√≠nh x√°c: 0.9856
B√°o c√°o ph√¢n lo·∫°i:
              precision    recall  f1-score   support

Kh√¥ng spam       0.99      0.99      0.99       966
Spam             0.97      0.97      0.97       149

    accuracy                           0.99      1115
   macro avg       0.98      0.98      0.98      1115
weighted avg       0.99      0.99      0.99      1115

Th·ªùi gian ch·∫°y: 180.45 gi√¢y
```

### **Gi·∫£i th√≠ch k·∫øt qu·∫£:**
- **Accuracy 98.56%:** Cao h∆°n TF-IDF approach
- **Better semantic understanding:** Hi·ªÉu ng·ªØ nghƒ©a s√¢u s·∫Øc h∆°n
- **Training time:** L√¢u h∆°n (3 ph√∫t) do ph·ª©c t·∫°p h∆°n

---

## **PH·∫¶N 4: DEMO USER INTERFACE (2 ph√∫t)**

### **B∆∞·ªõc 1: Ch·∫°y UI**
```bash
python ui_du_doan_email.py
```

### **Demo v·ªõi email m·∫´u:**

**Email spam m·∫´u:**
```
Subject: URGENT: You've won $1,000,000!
Body: Congratulations! You've been selected to receive $1,000,000. 
Click here to claim your prize: http://fake-spam-link.com
This is a limited time offer. Don't miss out!
```

**Email ham m·∫´u:**
```
Subject: Meeting tomorrow
Body: Hi team,
Just a reminder that we have a meeting tomorrow at 2 PM in the conference room.
Please prepare your quarterly reports.
Best regards,
John
```

### **Gi·∫£i th√≠ch UI:**
- **User-friendly interface:** D·ªÖ s·ª≠ d·ª•ng v·ªõi Tkinter
- **Real-time prediction:** K·∫øt qu·∫£ hi·ªÉn th·ªã ngay l·∫≠p t·ª©c
- **Error handling:** Th√¥ng b√°o l·ªói th√¢n thi·ªán
- **Clear instructions:** H∆∞·ªõng d·∫´n r√µ r√†ng

---

## **PH·∫¶N 5: SO S√ÅNH V√Ä K·∫æT LU·∫¨N (2 ph√∫t)**

### **B·∫£ng so s√°nh performance:**

| Metric | TF-IDF + LR | SentenceTransformer + LR |
|--------|-------------|-------------------------|
| Accuracy | 97.45% | 98.56% |
| Precision | 95% | 97% |
| Recall | 95% | 97% |
| F1-score | 95% | 97% |
| Training time | 15s | 180s |
| Memory usage | Th·∫•p | Cao |
| Interpretability | Cao | Th·∫•p |

### **∆Øu nh∆∞·ª£c ƒëi·ªÉm:**

**TF-IDF + Logistic Regression:**
- ‚úÖ Nhanh, ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu
- ‚úÖ Memory efficient
- ‚úÖ D·ªÖ interpret
- ‚ùå Kh√¥ng hi·ªÉu ng·ªØ nghƒ©a s√¢u s·∫Øc

**SentenceTransformer + Logistic Regression:**
- ‚úÖ Hi·ªÉu ng·ªØ nghƒ©a t·ªët h∆°n
- ‚úÖ Accuracy cao h∆°n
- ‚úÖ X·ª≠ l√Ω ƒë∆∞·ª£c context
- ‚ùå Ch·∫≠m h∆°n, ph·ª©c t·∫°p h∆°n

### **Recommendations:**
1. **Production:** S·ª≠ d·ª•ng SentenceTransformer cho accuracy cao
2. **Development:** S·ª≠ d·ª•ng TF-IDF cho rapid prototyping
3. **Resource-constrained:** TF-IDF ph√π h·ª£p h∆°n
4. **High-accuracy requirement:** SentenceTransformer l√† l·ª±a ch·ªçn t·ªët

---

## **PH·∫¶N 6: Q&A PREPARATION**

### **C√¢u h·ªèi th∆∞·ªùng g·∫∑p:**

**Q: "T·∫°i sao ch·ªçn Logistic Regression?"**
A: "Logistic Regression ph√π h·ª£p cho binary classification, nhanh, d·ªÖ interpret, v√† √≠t overfitting. ƒê·∫∑c bi·ªát t·ªët cho spam detection v√¨ ch√∫ng ta c·∫ßn hi·ªÉu ƒë∆∞·ª£c feature importance."

**Q: "L√†m sao c·∫£i thi·ªán model?"**
A: "C√≥ th·ªÉ th·ª≠: 1) Ensemble methods k·∫øt h·ª£p nhi·ªÅu models, 2) Deep learning (LSTM/BERT), 3) Feature engineering t·ªët h∆°n, 4) Data augmentation, 5) Hyperparameter tuning."

**Q: "Model c√≥ bias kh√¥ng?"**
A: "C√≥ th·ªÉ c√≥ bias do imbalanced dataset (13% spam, 87% ham). C·∫ßn x·ª≠ l√Ω b·∫±ng: 1) Balanced sampling, 2) Diverse training data, 3) Bias detection tools."

**Q: "L√†m sao deploy production?"**
A: "1) API development v·ªõi Flask/FastAPI, 2) Docker containerization, 3) Cloud deployment (AWS/GCP), 4) Monitoring v√† logging, 5) Auto-scaling."

---

## **PH·∫¶N 7: TECHNICAL DETAILS**

### **Code highlights:**

**Data preprocessing (tien_xu_ly.py):**
```python
def doc_va_tien_xu_ly_du_lieu(duong_dan_file: str):
    # Handle encoding issues
    try:
        du_lieu = pd.read_csv(duong_dan_file, encoding='utf-8')
    except UnicodeDecodeError:
        du_lieu = pd.read_csv(duong_dan_file, encoding='latin1')
    
    # Clean and prepare data
    du_lieu = du_lieu.rename(columns={'v1': 'nhan', 'v2': 'noi_dung'})
    du_lieu = du_lieu.dropna()
    du_lieu['nhan'] = pd.Series(du_lieu['nhan']).astype(str).replace({'ham': 0, 'spam': 1})
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        du_lieu['noi_dung'], du_lieu['nhan'], 
        test_size=0.2, random_state=42, stratify=du_lieu['nhan']
    )
    return X_train, X_test, y_train, y_test
```

**TF-IDF feature extraction (dac_trung.py):**
```python
def trich_xuat_tfidf(X_train, X_test) -> Tuple[csr_matrix, csr_matrix, TfidfVectorizer]:
    bo_tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=3000)
    X_train_tfidf = bo_tfidf.fit_transform(X_train)
    X_test_tfidf = bo_tfidf.transform(X_test)
    return csr_matrix(X_train_tfidf), csr_matrix(X_test_tfidf), bo_tfidf
```

**SentenceTransformer approach (mo_hinh.py):**
```python
def batch_encode(model, texts, batch_size=128):
    """Encode embedding theo batch nh·ªè ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ."""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        emb = model.encode(batch, show_progress_bar=False)
        embeddings.append(emb)
    return np.vstack(embeddings)
```

---

## **PH·∫¶N 8: CONCLUSION**

### **T√≥m t·∫Øt:**
- ‚úÖ X√¢y d·ª±ng th√†nh c√¥ng h·ªá th·ªëng spam detection
- ‚úÖ So s√°nh 2 approaches kh√°c nhau
- ‚úÖ ƒê·∫°t accuracy cao (97-99%)
- ‚úÖ C√≥ user interface th√¢n thi·ªán
- ‚úÖ Code modular v√† maintainable

### **Future work:**
- Ensemble methods
- Deep learning approaches
- Production deployment
- Real-time monitoring
- Continuous learning

### **Thank you:**
"C·∫£m ∆°n th·∫ßy/c√¥ v√† c√°c b·∫°n ƒë√£ l·∫Øng nghe. T√¥i s·∫µn s√†ng tr·∫£ l·ªùi c√°c c√¢u h·ªèi."

---

## **CHECKLIST TR∆Ø·ªöC KHI DEMO:**

- [ ] Test t·∫•t c·∫£ code tr∆∞·ªõc khi demo
- [ ] Chu·∫©n b·ªã email m·∫´u (spam v√† ham)
- [ ] Backup d·ªØ li·ªáu v√† models
- [ ] Practice demo nhi·ªÅu l·∫ßn
- [ ] Chu·∫©n b·ªã answers cho Q&A
- [ ] Test UI tr√™n m√°y kh√°c
- [ ] Backup presentation materials
- [ ] Chu·∫©n b·ªã slides ho·∫∑c notes

**Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ** 