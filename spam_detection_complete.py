#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“§ Há»† THá»NG NHáº¬N DIá»†N EMAIL SPAM
======================================

Dá»± Ã¡n Machine Learning sá»­ dá»¥ng SentenceTransformer vÃ  Logistic Regression
Ä‘á»ƒ phÃ¢n loáº¡i email spam vs ham.

TÃ¡c giáº£: DemoAI
NgÃ y táº¡o: 2024
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sentence_transformers import SentenceTransformer
import joblib
from collections import Counter
import re
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# Thiáº¿t láº­p font cho tiáº¿ng Viá»‡t
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'SimHei']
plt.style.use('seaborn-v0_8')

class SpamDetector:
    """Lá»›p chÃ­nh Ä‘á»ƒ xÃ¢y dá»±ng vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh nháº­n diá»‡n spam."""
    
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        self.model_name = model_name
        self.embedder = None
        self.model = None
        self.df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
    def load_data(self, file_path='spam.csv'):
        """Äá»c vÃ  táº£i dá»¯ liá»‡u tá»« file CSV."""
        print("ğŸ“Š KHÃM PHÃ VÃ€ PHÃ‚N TÃCH Dá»® LIá»†U EMAIL SPAM")
        print("=" * 50)
        
        try:
            self.df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            self.df = pd.read_csv(file_path, encoding='latin-1')
            
        print(f"Tá»•ng sá»‘ dÃ²ng dá»¯ liá»‡u: {len(self.df)}")
        return self.df
    
    def analyze_data(self):
        """PhÃ¢n tÃ­ch dá»¯ liá»‡u cÆ¡ báº£n."""
        if self.df is None:
            print("âŒ ChÆ°a táº£i dá»¯ liá»‡u. HÃ£y gá»i load_data() trÆ°á»›c.")
            return
            
        print("\nğŸ” THá»NG KÃŠ CÆ  Báº¢N:")
        print("-" * 30)
        
        # PhÃ¢n loáº¡i ham/spam
        spam_count = len(self.df[self.df['v1'] == 'spam'])
        ham_count = len(self.df[self.df['v1'] == 'ham'])
        
        print(f"Sá»‘ lÆ°á»£ng email HAM: {ham_count}")
        print(f"Sá»‘ lÆ°á»£ng email SPAM: {spam_count}")
        print(f"Tá»· lá»‡ HAM: {ham_count/(ham_count+spam_count)*100:.1f}%")
        print(f"Tá»· lá»‡ SPAM: {spam_count/(ham_count+spam_count)*100:.1f}%")
        
        # PhÃ¢n tÃ­ch Ä‘á»™ dÃ i tin nháº¯n
        self.df['length'] = self.df['v2'].str.len()
        ham_lengths = self.df[self.df['v1'] == 'ham']['length']
        spam_lengths = self.df[self.df['v1'] == 'spam']['length']
        
        print(f"\nğŸ“ PHÃ‚N TÃCH Äá»˜ DÃ€I TIN NHáº®N:")
        print(f"Äá»™ dÃ i trung bÃ¬nh HAM: {ham_lengths.mean():.1f} kÃ½ tá»±")
        print(f"Äá»™ dÃ i trung bÃ¬nh SPAM: {spam_lengths.mean():.1f} kÃ½ tá»±")
        print(f"Äá»™ dÃ i tá»‘i Ä‘a HAM: {ham_lengths.max()} kÃ½ tá»±")
        print(f"Äá»™ dÃ i tá»‘i Ä‘a SPAM: {spam_lengths.max()} kÃ½ tá»±")
        
        return ham_count, spam_count, ham_lengths, spam_lengths
    
    def analyze_keywords(self):
        """PhÃ¢n tÃ­ch tá»« khÃ³a trong dá»¯ liá»‡u."""
        print(f"\nğŸ”¤ PHÃ‚N TÃCH Tá»ª KHÃ“A THÆ¯á»œNG XUáº¤T HIá»†N:")
        
        def get_keywords(text):
            words = re.findall(r'\b\w+\b', text.lower())
            return [word for word in words if len(word) > 2]
        
        # Tá»« khÃ³a trong SPAM
        spam_texts = ' '.join(self.df[self.df['v1'] == 'spam']['v2'].astype(str))
        spam_words = get_keywords(spam_texts)
        spam_word_freq = Counter(spam_words).most_common(10)
        
        print("Top 10 tá»« khÃ³a trong SPAM:")
        for word, count in spam_word_freq:
            print(f"  - {word}: {count} láº§n")
        
        # Tá»« khÃ³a trong HAM
        ham_texts = ' '.join(self.df[self.df['v1'] == 'ham']['v2'].astype(str))
        ham_words = get_keywords(ham_texts)
        ham_word_freq = Counter(ham_words).most_common(10)
        
        print("\nTop 10 tá»« khÃ³a trong HAM:")
        for word, count in ham_word_freq:
            print(f"  - {word}: {count} láº§n")
            
        return spam_word_freq, ham_word_freq
    
    def create_visualizations(self, ham_count, spam_count, ham_lengths, spam_lengths, 
                            spam_word_freq, ham_word_freq):
        """Táº¡o biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch dá»¯ liá»‡u."""
        plt.figure(figsize=(15, 10))
        
        # Biá»ƒu Ä‘á»“ phÃ¢n loáº¡i
        plt.subplot(2, 3, 1)
        labels = ['HAM', 'SPAM']
        sizes = [ham_count, spam_count]
        colors = ['#66b3ff', '#ff9999']
        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('PhÃ¢n bá»‘ HAM vs SPAM')
        
        # Biá»ƒu Ä‘á»“ Ä‘á»™ dÃ i
        plt.subplot(2, 3, 2)
        plt.hist(ham_lengths, alpha=0.7, label='HAM', bins=30, color='blue')
        plt.hist(spam_lengths, alpha=0.7, label='SPAM', bins=30, color='red')
        plt.xlabel('Äá»™ dÃ i tin nháº¯n')
        plt.ylabel('Táº§n suáº¥t')
        plt.title('PhÃ¢n bá»‘ Ä‘á»™ dÃ i tin nháº¯n')
        plt.legend()
        
        # Box plot Ä‘á»™ dÃ i
        plt.subplot(2, 3, 3)
        data = [ham_lengths, spam_lengths]
        plt.boxplot(data, labels=['HAM', 'SPAM'])
        plt.ylabel('Äá»™ dÃ i tin nháº¯n')
        plt.title('Box Plot Ä‘á»™ dÃ i tin nháº¯n')
        
        # Tá»« khÃ³a SPAM
        plt.subplot(2, 3, 4)
        words, counts = zip(*spam_word_freq[:8])
        plt.barh(words, counts, color='red', alpha=0.7)
        plt.xlabel('Táº§n suáº¥t')
        plt.title('Top tá»« khÃ³a SPAM')
        
        # Tá»« khÃ³a HAM
        plt.subplot(2, 3, 5)
        words, counts = zip(*ham_word_freq[:8])
        plt.barh(words, counts, color='blue', alpha=0.7)
        plt.xlabel('Táº§n suáº¥t')
        plt.title('Top tá»« khÃ³a HAM')
        
        # Word Cloud SPAM
        plt.subplot(2, 3, 6)
        spam_texts = ' '.join(self.df[self.df['v1'] == 'spam']['v2'].astype(str))
        wordcloud = WordCloud(width=400, height=200, background_color='white', 
                             colormap='Reds', max_words=50).generate(spam_texts)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('Word Cloud SPAM')
        
        plt.tight_layout()
        plt.savefig('thong_ke_du_lieu.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"\nğŸ“ˆ Ã NGHÄ¨A VÃ€ áº¢NH HÆ¯á»NG:")
        print("-" * 30)
        print("1. Dá»¯ liá»‡u cÃ³ sá»± máº¥t cÃ¢n báº±ng nháº¹ giá»¯a HAM vÃ  SPAM")
        print("2. Tin nháº¯n SPAM thÆ°á»ng dÃ i hÆ¡n vÃ  chá»©a nhiá»u tá»« khÃ³a quáº£ng cÃ¡o")
        print("3. CÃ¡c tá»« khÃ³a nhÆ° 'free', 'win', 'click', 'offer' xuáº¥t hiá»‡n nhiá»u trong SPAM")
        print("4. Dá»¯ liá»‡u nÃ y giÃºp hiá»ƒu rÃµ hÆ¡n vá» Ä‘áº·c Ä‘iá»ƒm cá»§a email spam")
        print("5. CÃ³ thá»ƒ sá»­ dá»¥ng cÃ¡c tá»« khÃ³a nÃ y lÃ m features cho mÃ´ hÃ¬nh")
    
    def preprocess_data(self):
        """Tiá»n xá»­ lÃ½ dá»¯ liá»‡u cho mÃ´ hÃ¬nh."""
        print("ğŸ”§ TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
        print("=" * 30)
        
        # Äá»•i tÃªn cá»™t cho dá»… xá»­ lÃ½
        du_lieu = self.df.rename(columns={'v1': 'nhan', 'v2': 'noi_dung'})
        
        # Chá»‰ giá»¯ 2 cá»™t cáº§n thiáº¿t
        du_lieu = du_lieu[['nhan', 'noi_dung']]
        
        # Loáº¡i bá» dÃ²ng bá»‹ thiáº¿u dá»¯ liá»‡u
        du_lieu = du_lieu.dropna()
        
        # Äáº£m báº£o cá»™t 'nhan' lÃ  Series, dÃ¹ng .replace Ä‘Ãºng chuáº©n
        du_lieu['nhan'] = pd.Series(du_lieu['nhan']).astype(str).replace({'ham': 0, 'spam': 1})
        
        # TÃ¡ch táº­p train/test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            du_lieu['noi_dung'], du_lieu['nhan'], test_size=0.2, random_state=42, stratify=du_lieu['nhan']
        )
        
        print(f"KÃ­ch thÆ°á»›c táº­p train: {len(self.X_train)} máº«u")
        print(f"KÃ­ch thÆ°á»›c táº­p test: {len(self.X_test)} máº«u")
        print(f"Tá»· lá»‡ spam trong train: {self.y_train.mean():.3f}")
        print(f"Tá»· lá»‡ spam trong test: {self.y_test.mean():.3f}")
        
        # Hiá»ƒn thá»‹ má»™t sá»‘ máº«u
        print("\nğŸ“ Má»˜T Sá» MáºªU Dá»® LIá»†U:")
        for i in range(3):
            print(f"\nMáº«u {i+1}:")
            print(f"Ná»™i dung: {self.X_train.iloc[i][:100]}...")
            print(f"NhÃ£n: {'SPAM' if self.y_train.iloc[i] == 1 else 'HAM'}")
    
    def clean_text_list(self, series):
        """LÃ m sáº¡ch dá»¯ liá»‡u Ä‘áº§u vÃ o."""
        return [str(s) if pd.notnull(s) and str(s).strip() != "" else "[EMPTY]" for s in series]
    
    def batch_encode(self, texts, batch_size=128):
        """Encode embedding theo batch nhá» Ä‘á»ƒ trÃ¡nh trÃ n bá»™ nhá»›."""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            emb = self.embedder.encode(batch, show_progress_bar=False)
            embeddings.append(emb)
        return np.vstack(embeddings)
    
    def train_model(self):
        """Huáº¥n luyá»‡n mÃ´ hÃ¬nh."""
        print("ğŸ¤– XÃ‚Y Dá»°NG MÃ” HÃŒNH Vá»šI SENTENCETRANSFORMER")
        print("=" * 50)
        
        # Táº£i SentenceTransformer model
        print("ğŸ“¥ Äang táº£i SentenceTransformer model...")
        self.embedder = SentenceTransformer(self.model_name)
        print(f"âœ… ÄÃ£ táº£i model: {self.model_name}")
        
        # Tiá»n xá»­ lÃ½ vÃ  táº¡o embedding
        print("\nğŸ”„ Äang táº¡o embedding cho dá»¯ liá»‡u...")
        X_train_clean = self.clean_text_list(self.X_train)
        X_test_clean = self.clean_text_list(self.X_test)
        
        print("   - Äang encode táº­p train...")
        X_train_emb = self.batch_encode(X_train_clean)
        print("   - Äang encode táº­p test...")
        X_test_emb = self.batch_encode(X_test_clean)
        
        print(f"âœ… HoÃ n thÃ nh! KÃ­ch thÆ°á»›c embedding: {X_train_emb.shape[1]} chiá»u")
        
        # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
        print("\nğŸ¯ HUáº¤N LUYá»†N MÃ” HÃŒNH LOGISTIC REGRESSION")
        print("=" * 45)
        
        self.model = LogisticRegression(max_iter=1000)
        self.model.fit(X_train_emb, self.y_train)
        print("âœ… HoÃ n thÃ nh huáº¥n luyá»‡n!")
        
        # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
        print("\nğŸ“Š ÄÃNH GIÃ MÃ” HÃŒNH")
        print("=" * 20)
        
        y_pred = self.model.predict(X_test_emb)
        do_chinh_xac = accuracy_score(self.y_test, y_pred)
        bao_cao = classification_report(self.y_test, y_pred, target_names=['KhÃ´ng pháº£i rÃ¡c', 'ThÆ° rÃ¡c'])
        
        print(f"Äá»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ: {do_chinh_xac:.4f}")
        print("\nBÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t:")
        print(bao_cao)
        
        return do_chinh_xac, bao_cao
    
    def predict_email(self, email_text):
        """Dá»± Ä‘oÃ¡n má»™t email lÃ  spam hay khÃ´ng spam."""
        if self.model is None or self.embedder is None:
            print("âŒ MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i train_model() trÆ°á»›c.")
            return None
            
        email_clean = self.clean_text_list([email_text])
        email_emb = self.batch_encode(email_clean)
        prediction = self.model.predict(email_emb)[0]
        return "Spam" if prediction == 1 else "KhÃ´ng spam"
    
    def save_model(self, model_path='mo_hinh_spam.pkl', embedder_path='sentence_model.txt'):
        """LÆ°u mÃ´ hÃ¬nh vÃ  tÃªn model embedding vÃ o file."""
        if self.model is None:
            print("âŒ MÃ´ hÃ¬nh chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i train_model() trÆ°á»›c.")
            return
            
        print("ğŸ’¾ LÆ¯U MÃ” HÃŒNH")
        print("=" * 15)
        
        joblib.dump(self.model, model_path)
        with open(embedder_path, 'w', encoding='utf-8') as f:
            f.write(self.model_name)
            
        print(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o '{model_path}'")
        print(f"âœ… ÄÃ£ lÆ°u tÃªn model vÃ o '{embedder_path}'")
    
    def load_saved_model(self, model_path='mo_hinh_spam.pkl', embedder_path='sentence_model.txt'):
        """Táº£i mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u tá»« file."""
        print("ğŸ“¥ Táº¢I MÃ” HÃŒNH ÄÃƒ LÆ¯U")
        print("=" * 25)
        
        try:
            self.model = joblib.load(model_path)
            with open(embedder_path, 'r', encoding='utf-8') as f:
                model_name = f.read().strip()
            self.embedder = SentenceTransformer(model_name)
            print("âœ… ÄÃ£ táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng!")
            return True
        except Exception as e:
            print(f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh: {e}")
            return False
    
    def demo_predictions(self):
        """Demo dá»± Ä‘oÃ¡n vá»›i cÃ¡c vÃ­ dá»¥."""
        print("ğŸ§ª DEMO Dá»° ÄOÃN EMAIL SPAM")
        print("=" * 30)
        
        test_emails = [
            "Hello, how are you? I hope you're doing well.",
            "FREE! WIN A PRIZE! CLICK HERE NOW! LIMITED TIME OFFER!",
            "Meeting tomorrow at 3 PM. Please bring the documents.",
            "CONGRATULATIONS! You've won $1000! Claim your prize now!",
            "Hi mom, I'll be home late tonight. Love you!"
        ]
        
        print("ğŸ“§ Káº¾T QUáº¢ Dá»° ÄOÃN:")
        print("-" * 25)
        
        for i, email in enumerate(test_emails, 1):
            ket_qua = self.predict_email(email)
            print(f"\n{i}. Email: {email[:50]}...")
            print(f"   Káº¿t quáº£: {ket_qua}")
    
    def run_complete_pipeline(self, data_path='spam.csv'):
        """Cháº¡y toÃ n bá»™ pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i."""
        print("ğŸš€ CHáº Y TOÃ€N Bá»˜ PIPELINE SPAM DETECTION")
        print("=" * 50)
        
        # 1. Táº£i dá»¯ liá»‡u
        self.load_data(data_path)
        
        # 2. PhÃ¢n tÃ­ch dá»¯ liá»‡u
        ham_count, spam_count, ham_lengths, spam_lengths = self.analyze_data()
        spam_word_freq, ham_word_freq = self.analyze_keywords()
        
        # 3. Táº¡o biá»ƒu Ä‘á»“
        self.create_visualizations(ham_count, spam_count, ham_lengths, spam_lengths,
                                 spam_word_freq, ham_word_freq)
        
        # 4. Tiá»n xá»­ lÃ½
        self.preprocess_data()
        
        # 5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
        accuracy, report = self.train_model()
        
        # 6. LÆ°u mÃ´ hÃ¬nh
        self.save_model()
        
        # 7. Demo
        self.demo_predictions()
        
        print("\nğŸ‰ HOÃ€N THÃ€NH PIPELINE!")
        print(f"Äá»™ chÃ­nh xÃ¡c cuá»‘i cÃ¹ng: {accuracy:.4f}")
        
        return accuracy, report


def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y dá»± Ã¡n."""
    print("ğŸ“§ Há»† THá»NG NHáº¬N DIá»†N EMAIL SPAM")
    print("=" * 50)
    print("TÃ¡c giáº£: DemoAI")
    print("Sá»­ dá»¥ng SentenceTransformer + Logistic Regression")
    print()
    
    # Táº¡o instance cá»§a SpamDetector
    detector = SpamDetector()
    
    # Cháº¡y toÃ n bá»™ pipeline
    try:
        accuracy, report = detector.run_complete_pipeline()
        
        print("\n" + "="*50)
        print("ğŸ“‹ TÃ“M Táº®T Káº¾T QUáº¢:")
        print(f"âœ… Äá»™ chÃ­nh xÃ¡c: {accuracy:.4f}")
        print("âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o 'mo_hinh_spam.pkl'")
        print("âœ… Biá»ƒu Ä‘á»“ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o 'thong_ke_du_lieu.png'")
        print("âœ… CÃ³ thá»ƒ sá»­ dá»¥ng detector.predict_email() Ä‘á»ƒ dá»± Ä‘oÃ¡n email má»›i")
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o file spam.csv cÃ³ trong thÆ° má»¥c hiá»‡n táº¡i")


if __name__ == "__main__":
    main() 